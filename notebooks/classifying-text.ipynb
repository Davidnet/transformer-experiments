{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "\n",
    "from flax import linen as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement a Transformer block as a layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    \"\"\"Transformer block.\"\"\"\n",
    "    embed_dim: int\n",
    "    num_heads: int\n",
    "    ff_dim: int\n",
    "    rate: float = 0.1\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x, training: bool):\n",
    "        attn_output = nn.SelfAttention(num_heads=self.num_heads, qkv_features=self.embed_dim)(x)\n",
    "        attn_output = nn.Dropout(rate=self.rate, deterministic = not training)(attn_output)\n",
    "        out1 = x + attn_output\n",
    "        out1 = nn.LayerNorm()(out1)\n",
    "        ffn_output = nn.relu(nn.Dense(self.ff_dim)(out1))\n",
    "        ffn_output = nn.Dense(self.embed_dim)(ffn_output)\n",
    "        ffn_output = nn.Dropout(rate=self.rate, deterministic = not training)(ffn_output)\n",
    "        return nn.LayerNorm()(out1 + ffn_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement embedding layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenAndPositionEmbedding(nn.Module):\n",
    "    \"\"\" Combine token embedding and position embedding.\"\"\"\n",
    "    maxlen: int\n",
    "    vocab_size: int\n",
    "    embed_dim: int\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x):\n",
    "        positions = jnp.expand_dims(jnp.arange(x.shape[1]), axis=0)\n",
    "        pos_emb = nn.Embed(num_embeddings=self.maxlen, features=self.embed_dim)(positions)\n",
    "        tok_emb = nn.Embed(num_embeddings=self.vocab_size, features=self.embed_dim)(x)\n",
    "        return pos_emb + tok_emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    \"\"\"Transformer model.\"\"\"\n",
    "    num_classes: int\n",
    "    vocab_size: int\n",
    "    maxlen: int\n",
    "    embed_dim: int\n",
    "    num_heads: int\n",
    "    ff_dim: int\n",
    "    rate: float = 0.1\n",
    "    \n",
    "    @nn.compact\n",
    "    def __call__(self, x, training: bool):\n",
    "        x = TokenAndPositionEmbedding(maxlen=self.maxlen, vocab_size=self.vocab_size, embed_dim=self.embed_dim)(x)\n",
    "        x = TransformerBlock(embed_dim=self.embed_dim, num_heads=self.num_heads, ff_dim=self.ff_dim, rate=self.rate)(x, training=training)\n",
    "        x = jnp.mean(x, axis=1) # GlobalAveragePooling1D ?\n",
    "        x = nn.Dropout(rate=self.rate, deterministic = not training)(x)\n",
    "        x = nn.Dense(20)(x)\n",
    "        x = nn.relu(x)\n",
    "        x = nn.Dropout(rate=self.rate, deterministic = not training)(x)\n",
    "        x = nn.Dense(self.num_classes)(x)\n",
    "        # x = nn.log_softmax(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 20000  # Only consider the top 20k words\n",
    "maxlen = 200  # Only consider the first 200 words of each movie review\n",
    "embed_dim = 32  # Embedding size for each token\n",
    "num_heads = 2  # Number of attention heads\n",
    "ff_dim = 32  # Hidden layer size in feed forward network inside transformer\n",
    "transformer = Transformer(num_classes=2, vocab_size=vocab_size, maxlen=maxlen, embed_dim=embed_dim, num_heads=num_heads, ff_dim=ff_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[3m                              Transformer Summary                               \u001b[0m\n",
      "┏━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━┓\n",
      "┃\u001b[1m \u001b[0m\u001b[1mpath         \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mmodule       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1minputs       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1moutputs      \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mparams      \u001b[0m\u001b[1m \u001b[0m┃\n",
      "┡━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━┩\n",
      "│               │ Transformer   │ -             │ \u001b[2mfloat32\u001b[0m[1,2]  │              │\n",
      "│               │               │ \u001b[2mint32\u001b[0m[1,200]  │               │              │\n",
      "│               │               │ - training:   │               │              │\n",
      "│               │               │ False         │               │              │\n",
      "├───────────────┼───────────────┼───────────────┼───────────────┼──────────────┤\n",
      "│ TokenAndPosi… │ TokenAndPosi… │ \u001b[2mint32\u001b[0m[1,200]  │ \u001b[2mfloat32\u001b[0m[1,20… │              │\n",
      "├───────────────┼───────────────┼───────────────┼───────────────┼──────────────┤\n",
      "│ TokenAndPosi… │ Embed         │ \u001b[2mint32\u001b[0m[1,200]  │ \u001b[2mfloat32\u001b[0m[1,20… │ embedding:   │\n",
      "│               │               │               │               │ \u001b[2mfloat32\u001b[0m[200… │\n",
      "│               │               │               │               │              │\n",
      "│               │               │               │               │ \u001b[1m6,400 \u001b[0m\u001b[1;2m(25.6 \u001b[0m │\n",
      "│               │               │               │               │ \u001b[1;2mKB)\u001b[0m          │\n",
      "├───────────────┼───────────────┼───────────────┼───────────────┼──────────────┤\n",
      "│ TokenAndPosi… │ Embed         │ \u001b[2mint32\u001b[0m[1,200]  │ \u001b[2mfloat32\u001b[0m[1,20… │ embedding:   │\n",
      "│               │               │               │               │ \u001b[2mfloat32\u001b[0m[200… │\n",
      "│               │               │               │               │              │\n",
      "│               │               │               │               │ \u001b[1m640,000 \u001b[0m\u001b[1;2m(2.6\u001b[0m │\n",
      "│               │               │               │               │ \u001b[1;2mMB)\u001b[0m          │\n",
      "├───────────────┼───────────────┼───────────────┼───────────────┼──────────────┤\n",
      "│ TransformerB… │ TransformerB… │ -             │ \u001b[2mfloat32\u001b[0m[1,20… │              │\n",
      "│               │               │ \u001b[2mfloat32\u001b[0m[1,20… │               │              │\n",
      "│               │               │ - training:   │               │              │\n",
      "│               │               │ False         │               │              │\n",
      "├───────────────┼───────────────┼───────────────┼───────────────┼──────────────┤\n",
      "│ TransformerB… │ SelfAttention │ \u001b[2mfloat32\u001b[0m[1,20… │ \u001b[2mfloat32\u001b[0m[1,20… │              │\n",
      "├───────────────┼───────────────┼───────────────┼───────────────┼──────────────┤\n",
      "│ TransformerB… │ DenseGeneral  │ \u001b[2mfloat32\u001b[0m[1,20… │ \u001b[2mfloat32\u001b[0m[1,20… │ bias:        │\n",
      "│               │               │               │               │ \u001b[2mfloat32\u001b[0m[2,1… │\n",
      "│               │               │               │               │ kernel:      │\n",
      "│               │               │               │               │ \u001b[2mfloat32\u001b[0m[32,… │\n",
      "│               │               │               │               │              │\n",
      "│               │               │               │               │ \u001b[1m1,056 \u001b[0m\u001b[1;2m(4.2 \u001b[0m  │\n",
      "│               │               │               │               │ \u001b[1;2mKB)\u001b[0m          │\n",
      "├───────────────┼───────────────┼───────────────┼───────────────┼──────────────┤\n",
      "│ TransformerB… │ DenseGeneral  │ \u001b[2mfloat32\u001b[0m[1,20… │ \u001b[2mfloat32\u001b[0m[1,20… │ bias:        │\n",
      "│               │               │               │               │ \u001b[2mfloat32\u001b[0m[2,1… │\n",
      "│               │               │               │               │ kernel:      │\n",
      "│               │               │               │               │ \u001b[2mfloat32\u001b[0m[32,… │\n",
      "│               │               │               │               │              │\n",
      "│               │               │               │               │ \u001b[1m1,056 \u001b[0m\u001b[1;2m(4.2 \u001b[0m  │\n",
      "│               │               │               │               │ \u001b[1;2mKB)\u001b[0m          │\n",
      "├───────────────┼───────────────┼───────────────┼───────────────┼──────────────┤\n",
      "│ TransformerB… │ DenseGeneral  │ \u001b[2mfloat32\u001b[0m[1,20… │ \u001b[2mfloat32\u001b[0m[1,20… │ bias:        │\n",
      "│               │               │               │               │ \u001b[2mfloat32\u001b[0m[2,1… │\n",
      "│               │               │               │               │ kernel:      │\n",
      "│               │               │               │               │ \u001b[2mfloat32\u001b[0m[32,… │\n",
      "│               │               │               │               │              │\n",
      "│               │               │               │               │ \u001b[1m1,056 \u001b[0m\u001b[1;2m(4.2 \u001b[0m  │\n",
      "│               │               │               │               │ \u001b[1;2mKB)\u001b[0m          │\n",
      "├───────────────┼───────────────┼───────────────┼───────────────┼──────────────┤\n",
      "│ TransformerB… │ DenseGeneral  │ \u001b[2mfloat32\u001b[0m[1,20… │ \u001b[2mfloat32\u001b[0m[1,20… │ bias:        │\n",
      "│               │               │               │               │ \u001b[2mfloat32\u001b[0m[32]  │\n",
      "│               │               │               │               │ kernel:      │\n",
      "│               │               │               │               │ \u001b[2mfloat32\u001b[0m[2,1… │\n",
      "│               │               │               │               │              │\n",
      "│               │               │               │               │ \u001b[1m1,056 \u001b[0m\u001b[1;2m(4.2 \u001b[0m  │\n",
      "│               │               │               │               │ \u001b[1;2mKB)\u001b[0m          │\n",
      "├───────────────┼───────────────┼───────────────┼───────────────┼──────────────┤\n",
      "│ TransformerB… │ Dropout       │ \u001b[2mfloat32\u001b[0m[1,20… │ \u001b[2mfloat32\u001b[0m[1,20… │              │\n",
      "├───────────────┼───────────────┼───────────────┼───────────────┼──────────────┤\n",
      "│ TransformerB… │ LayerNorm     │ \u001b[2mfloat32\u001b[0m[1,20… │ \u001b[2mfloat32\u001b[0m[1,20… │ bias:        │\n",
      "│               │               │               │               │ \u001b[2mfloat32\u001b[0m[32]  │\n",
      "│               │               │               │               │ scale:       │\n",
      "│               │               │               │               │ \u001b[2mfloat32\u001b[0m[32]  │\n",
      "│               │               │               │               │              │\n",
      "│               │               │               │               │ \u001b[1m64 \u001b[0m\u001b[1;2m(256 B)\u001b[0m   │\n",
      "├───────────────┼───────────────┼───────────────┼───────────────┼──────────────┤\n",
      "│ TransformerB… │ Dense         │ \u001b[2mfloat32\u001b[0m[1,20… │ \u001b[2mfloat32\u001b[0m[1,20… │ bias:        │\n",
      "│               │               │               │               │ \u001b[2mfloat32\u001b[0m[32]  │\n",
      "│               │               │               │               │ kernel:      │\n",
      "│               │               │               │               │ \u001b[2mfloat32\u001b[0m[32,… │\n",
      "│               │               │               │               │              │\n",
      "│               │               │               │               │ \u001b[1m1,056 \u001b[0m\u001b[1;2m(4.2 \u001b[0m  │\n",
      "│               │               │               │               │ \u001b[1;2mKB)\u001b[0m          │\n",
      "├───────────────┼───────────────┼───────────────┼───────────────┼──────────────┤\n",
      "│ TransformerB… │ Dense         │ \u001b[2mfloat32\u001b[0m[1,20… │ \u001b[2mfloat32\u001b[0m[1,20… │ bias:        │\n",
      "│               │               │               │               │ \u001b[2mfloat32\u001b[0m[32]  │\n",
      "│               │               │               │               │ kernel:      │\n",
      "│               │               │               │               │ \u001b[2mfloat32\u001b[0m[32,… │\n",
      "│               │               │               │               │              │\n",
      "│               │               │               │               │ \u001b[1m1,056 \u001b[0m\u001b[1;2m(4.2 \u001b[0m  │\n",
      "│               │               │               │               │ \u001b[1;2mKB)\u001b[0m          │\n",
      "├───────────────┼───────────────┼───────────────┼───────────────┼──────────────┤\n",
      "│ TransformerB… │ Dropout       │ \u001b[2mfloat32\u001b[0m[1,20… │ \u001b[2mfloat32\u001b[0m[1,20… │              │\n",
      "├───────────────┼───────────────┼───────────────┼───────────────┼──────────────┤\n",
      "│ TransformerB… │ LayerNorm     │ \u001b[2mfloat32\u001b[0m[1,20… │ \u001b[2mfloat32\u001b[0m[1,20… │ bias:        │\n",
      "│               │               │               │               │ \u001b[2mfloat32\u001b[0m[32]  │\n",
      "│               │               │               │               │ scale:       │\n",
      "│               │               │               │               │ \u001b[2mfloat32\u001b[0m[32]  │\n",
      "│               │               │               │               │              │\n",
      "│               │               │               │               │ \u001b[1m64 \u001b[0m\u001b[1;2m(256 B)\u001b[0m   │\n",
      "├───────────────┼───────────────┼───────────────┼───────────────┼──────────────┤\n",
      "│ Dropout_0     │ Dropout       │ \u001b[2mfloat32\u001b[0m[1,32] │ \u001b[2mfloat32\u001b[0m[1,32] │              │\n",
      "├───────────────┼───────────────┼───────────────┼───────────────┼──────────────┤\n",
      "│ Dense_0       │ Dense         │ \u001b[2mfloat32\u001b[0m[1,32] │ \u001b[2mfloat32\u001b[0m[1,20] │ bias:        │\n",
      "│               │               │               │               │ \u001b[2mfloat32\u001b[0m[20]  │\n",
      "│               │               │               │               │ kernel:      │\n",
      "│               │               │               │               │ \u001b[2mfloat32\u001b[0m[32,… │\n",
      "│               │               │               │               │              │\n",
      "│               │               │               │               │ \u001b[1m660 \u001b[0m\u001b[1;2m(2.6 KB)\u001b[0m │\n",
      "├───────────────┼───────────────┼───────────────┼───────────────┼──────────────┤\n",
      "│ Dropout_1     │ Dropout       │ \u001b[2mfloat32\u001b[0m[1,20] │ \u001b[2mfloat32\u001b[0m[1,20] │              │\n",
      "├───────────────┼───────────────┼───────────────┼───────────────┼──────────────┤\n",
      "│ Dense_1       │ Dense         │ \u001b[2mfloat32\u001b[0m[1,20] │ \u001b[2mfloat32\u001b[0m[1,2]  │ bias:        │\n",
      "│               │               │               │               │ \u001b[2mfloat32\u001b[0m[2]   │\n",
      "│               │               │               │               │ kernel:      │\n",
      "│               │               │               │               │ \u001b[2mfloat32\u001b[0m[20,… │\n",
      "│               │               │               │               │              │\n",
      "│               │               │               │               │ \u001b[1m42 \u001b[0m\u001b[1;2m(168 B)\u001b[0m   │\n",
      "├───────────────┼───────────────┼───────────────┼───────────────┼──────────────┤\n",
      "│\u001b[1m \u001b[0m\u001b[1m             \u001b[0m\u001b[1m \u001b[0m│\u001b[1m \u001b[0m\u001b[1m             \u001b[0m\u001b[1m \u001b[0m│\u001b[1m \u001b[0m\u001b[1m             \u001b[0m\u001b[1m \u001b[0m│\u001b[1m \u001b[0m\u001b[1m        Total\u001b[0m\u001b[1m \u001b[0m│\u001b[1m \u001b[0m\u001b[1m653,566 \u001b[0m\u001b[1;2m(2.6\u001b[0m\u001b[1m \u001b[0m│\n",
      "│\u001b[1m               \u001b[0m│\u001b[1m               \u001b[0m│\u001b[1m               \u001b[0m│\u001b[1m               \u001b[0m│\u001b[1m \u001b[0m\u001b[1;2mMB)\u001b[0m\u001b[1m         \u001b[0m\u001b[1m \u001b[0m│\n",
      "└───────────────┴───────────────┴───────────────┴───────────────┴──────────────┘\n",
      "\u001b[1m                                                                                \u001b[0m\n",
      "\u001b[1m                       Total Parameters: 653,566 \u001b[0m\u001b[1;2m(2.6 MB)\u001b[0m\u001b[1m                       \u001b[0m\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# print(transformer.tabulate(jax.random.PRNGKey(0), jnp.ones_like(x_train[:1])))\n",
    "print(transformer.tabulate(jax.random.PRNGKey(0), jnp.ones((1, 200), dtype=jnp.int32), training=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-19 00:44:22.929820: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-02-19 00:44:23.365061: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-02-19 00:44:23.365137: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-02-19 00:44:23.365144: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow import keras;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from clu import metrics\n",
    "from flax.training import train_state  # Useful dataclass to keep train state\n",
    "from flax import struct                # Flax dataclasses\n",
    "import optax                           # Common loss functions and optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "@struct.dataclass\n",
    "class Metrics(metrics.Collection):\n",
    "  accuracy: metrics.Accuracy\n",
    "  loss: metrics.Average.from_output('loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = jax.random.PRNGKey(0)\n",
    "transformer.init(rng, jnp.ones((1, 200), dtype=jnp.int32), training=False);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainState(train_state.TrainState):\n",
    "  metrics: Metrics\n",
    "  key: jax.random.KeyArray\n",
    "\n",
    "def create_train_state(module, root_key, learning_rate):\n",
    "  \"\"\"Creates an initial `TrainState`.\"\"\"\n",
    "  main_key, params_key, dropout_key = jax.random.split(key=root_key, num=3)\n",
    "  params = module.init(params_key, jnp.ones((1, 200), dtype=jnp.int32), training=False)['params'] # initialize parameters by passing a template image\n",
    "  tx = optax.adam(learning_rate=learning_rate)\n",
    "  return TrainState.create(\n",
    "      apply_fn=module.apply, params=params, tx=tx, key=dropout_key, metrics=Metrics.empty())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jax.jit\n",
    "def train_step(state: TrainState, batch, dropout_key):\n",
    "  dropout_train_key = jax.random.fold_in(key=dropout_key, data=state.step)\n",
    "  def loss_fn(params):\n",
    "    logits = state.apply_fn(\n",
    "      {'params': params},\n",
    "      x=batch['token'],\n",
    "      training=True,\n",
    "      rngs={'dropout': dropout_train_key}\n",
    "      )\n",
    "    loss = optax.softmax_cross_entropy_with_integer_labels(\n",
    "      logits=logits, labels=batch['label']).mean()\n",
    "    return loss, logits\n",
    "  \n",
    "  grad_fn = jax.value_and_grad(loss_fn, has_aux=True)\n",
    "  (loss, logits), grads = grad_fn(state.params)\n",
    "  state = state.apply_gradients(grads=grads)\n",
    "  return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jax.jit\n",
    "def compute_metrics(*, state, batch):\n",
    "  logits = state.apply_fn({'params': state.params}, x=batch['token'], training=False)\n",
    "  loss = optax.softmax_cross_entropy_with_integer_labels(\n",
    "        logits=logits, labels=batch['label']).mean()\n",
    "  metric_updates = state.metrics.single_from_model_output(\n",
    "    logits=logits, labels=batch['label'], loss=loss)\n",
    "  metrics = state.metrics.merge(metric_updates)\n",
    "  state = state.replace(metrics=metrics)\n",
    "  return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_key = jax.random.PRNGKey(seed=0)\n",
    "main_key, params_key, dropout_key = jax.random.split(key=root_key, num=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = create_train_state(transformer, params_key, learning_rate=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_history = {'train_loss': [],\n",
    "                   'train_accuracy': [],\n",
    "                   'test_loss': [],\n",
    "                   'test_accuracy': []}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24992 Training sequences\n",
      "24992 Validation sequences\n"
     ]
    }
   ],
   "source": [
    "\n",
    "(x_train, y_train), (x_val, y_val) = keras.datasets.imdb.load_data(num_words=vocab_size)\n",
    "\n",
    "x_train = keras.preprocessing.sequence.pad_sequences(x_train, maxlen=maxlen)\n",
    "x_val = keras.preprocessing.sequence.pad_sequences(x_val, maxlen=maxlen)\n",
    "\n",
    "# We are going to drop the last few samples to make the batch size even.\n",
    "batch_size = 32\n",
    "num_batches = len(x_train) // batch_size\n",
    "x_train = x_train[:num_batches * batch_size]\n",
    "y_train = y_train[:num_batches * batch_size]\n",
    "x_val = x_val[:num_batches * batch_size]\n",
    "y_val = y_val[:num_batches * batch_size]\n",
    "\n",
    "print(len(x_train), \"Training sequences\")\n",
    "print(len(x_val), \"Validation sequences\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train epoch: 1, loss: 0.3994245231151581, accuracy: 80.27368927001953\n",
      "test epoch: 1, loss: 0.33239904046058655, accuracy: 85.30729675292969\n",
      "train epoch: 2, loss: 0.20272627472877502, accuracy: 92.36555480957031\n",
      "test epoch: 2, loss: 0.35978537797927856, accuracy: 85.24327850341797\n",
      "train epoch: 3, loss: 0.139784038066864, accuracy: 95.1624526977539\n",
      "test epoch: 3, loss: 0.3845835328102112, accuracy: 85.91149139404297\n"
     ]
    }
   ],
   "source": [
    "\n",
    "num_steps_per_epoch = len(x_train) // batch_size\n",
    "for epoch in range(1,4):\n",
    "    for step, batch_ix in enumerate(range(0, len(x_train), 32)):\n",
    "        batch = {\"token\": jnp.array(x_train[batch_ix:batch_ix+32]), \"label\": jnp.array(y_train[batch_ix:batch_ix+32])}\n",
    "        state = train_step(state, batch, dropout_key=dropout_key)\n",
    "        state = compute_metrics(state=state, batch=batch) # aggregate batch metrics\n",
    "\n",
    "        if (step+1)*epoch % num_steps_per_epoch == 0: # one training epoch has passed\n",
    "            for metric,value in state.metrics.compute().items(): # compute metrics\n",
    "                metrics_history[f'train_{metric}'].append(value) # record metrics\n",
    "            state = state.replace(metrics=state.metrics.empty()) # reset train_metrics for next training epoch\n",
    "\n",
    "            # Compute metrics on the test set after each training epoch\n",
    "            test_state = state\n",
    "            # for test_batch_idx in test_ds.as_numpy_iterator():\n",
    "\n",
    "            test_batch = {\"token\": jnp.array(x_val), \"label\": jnp.array(y_val)}\n",
    "            test_state = compute_metrics(state=test_state, batch=test_batch)\n",
    "\n",
    "            for metric,value in test_state.metrics.compute().items():\n",
    "                metrics_history[f'test_{metric}'].append(value)\n",
    "\n",
    "            print(f\"train epoch: {((step+1)*epoch) // num_steps_per_epoch}, \"\n",
    "                f\"loss: {metrics_history['train_loss'][-1]}, \"\n",
    "                f\"accuracy: {metrics_history['train_accuracy'][-1] * 100}\")\n",
    "            print(f\"test epoch: {(step+1)*epoch // num_steps_per_epoch}, \"\n",
    "                f\"loss: {metrics_history['test_loss'][-1]}, \"\n",
    "                f\"accuracy: {metrics_history['test_accuracy'][-1] * 100}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "fda47d8a7cc3c6229d1fba7e74c69b8ee9fdda93d2186f1a02a8e2cf986b7d99"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
